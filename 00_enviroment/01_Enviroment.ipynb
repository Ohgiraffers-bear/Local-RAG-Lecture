{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe038dc7",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "Ollama는 로컬 환경에서 대형 언어 모델(LLM)을 손쉽게 실행할 수 있는 오픈소스 플랫폼입니다.\n",
    "다양한 AI 모델을 다운로드하고, 프롬프트를 통해 직접 대화하거나 활용할 수 있습니다.\n",
    "간단한 명령어로 모델 관리 및 실행이 가능하여 AI 실습과 개발에 매우 유용합니다.\n",
    "\n",
    "\n",
    "## 1. 설치\n",
    "### 프로그램 설치\n",
    "- 현재 학습하는 컴퓨터의 OS(Windows / Mac / Linux )에 맞춰 Ollama를 다운로드하고 설치합니다\n",
    "- https://ollama.com\n",
    "\n",
    "### Ollama에서 제공하는 모델\n",
    "- https://ollama.com/search 에 접속해보면 Ollama에서 제공하는 모델을 확인 할 수 있습니다.\n",
    "\n",
    "터미널에 `ollama pull [모델이름]` 명령을 사용해 LLM 모델을 설치할 수 있습니다.  \n",
    "이때, 모델이름 뒤에 :7b, :8b 이런 형태로 모델의 크기를 태그로 지정해서 가져올 수 있습니다.   \n",
    "크기가 클수록 성능이 좋아지고, 필요한 컴퓨터 사양이 증가합니다.\n",
    "- ex) `ollama pull deepseek-r1:7b`\n",
    "\n",
    "`ollama list` 명령을 사용해 설치된 LLM 모델 목록을 확인 할 수 있습니다.  \n",
    "\n",
    "`ollama run [모델이름]` 명령을 사용해 모델을 동작시킬 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35a03",
   "metadata": {},
   "source": [
    "## Ollama에서 Custom 모델 생성하기\n",
    "- Ollama에서 제공하는 모델이외의 모델을 추가로 등록해서 사용 할 수 있습니다.\n",
    "- 추가로 등록하기 위해서는 `Modelfile` 문서와 `.gguf`로 저장된 LLM 파일이 필요합니다.\n",
    "\n",
    "## Model 다운로드받기\n",
    "- 한국어 특화 모델인 KT에서 제공하는 Mi:dm2.0 모델을 사용할것입니다.\n",
    "\n",
    "### Mi:dm 2.0 모델이란?   \n",
    "Mi:dm 2.0은 KT의 독자 기술을 활용하여 개발된 \"한국 중심 AI\" 모델 입니다.  \n",
    "\"한국 중심 AI\" 는 한국 사회에 내재된 고유한 가치관, 인지 프레임워크, 그리고 상식적 추론을 깊이 내재화하는 모델을 의미합니다.  \n",
    "단순히 한국어 텍스트를 처리하거나 생성하는 것을 넘어, 한국 사회를 정의하는 사회문화적 규범과 가치에 대한 더 깊은 이해를 반영합니다.  \n",
    "[참고] - https://huggingface.co/K-intelligence/Midm-2.0-Base-Instruct\n",
    "\n",
    "\n",
    "### 1. GGUF 확장자 모델 파일 다운로드 받기\n",
    "https://huggingface.co/bearc/Midm-2.0-Base-Instruct-Q5_K_M-GGUF\n",
    "- 해당 huggingface 저장소에서 `.gguf` 파일 및 `Modelfile`을 다운로드합니다\n",
    "-> 추후 KT에서 GGUF를 정식 제공해주면 해당파일로 변경해도 괜찮습니다\n",
    "\n",
    "### 2. Modelfile 이란?\n",
    "Modelfile은 Ollama가 해당 모델을 어떻게 불러오고, 어떤 방식으로 입력과 출력을 처리할지 정의 하는 파일이다.  \n",
    "사용할 모델의 경로, 프롬프트 템플릿, 파라미터 등 모델 동작에 필요한 다양한 설정 정보를 담을 수 있다.  \n",
    "\n",
    "### 3. Ollama에 등록하기  \n",
    "- 터미널에서 Modelfile 위치로 이동 후 명령어 실행  \n",
    "기본 문법: `ollama create [모델명] -f [Modelfile경로]`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1694deb2",
   "metadata": {},
   "source": [
    "ollama 라이브러리를 이용해 테스트해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "prompt = \"대한민국 육군에 대해 3줄로 설명해줘\"\n",
    "# 한 줄로 끝나는 방법\n",
    "response = ollama.generate(model='midm-2.0-base-instruct-q5_k_m', prompt=prompt)\n",
    "\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d8f3a",
   "metadata": {},
   "source": [
    "함수로 바꾸기 -> Prompt_Engineering 에서 사용 할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa44f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "prompt = \"대한민국 육군에 대해 3줄로 설명해줘\"'\n",
    "\n",
    "def generate_response(prompt):\n",
    "    response = ollama.generate(model='midm-2.0-base-instruct-q5_k_m', prompt=prompt)\n",
    "    return response['response']\n",
    "\n",
    "print(generate_response(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LCEL Runnable í”„ë¡œí† ì½œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Runnable í”„ë¡œí† ì½œ ì†Œê°œ\n",
        "\n",
        "### Runnableì´ë€?\n",
        "- **Runnable**ì€ LangChainì˜ í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ë¡œ, ì‘ì—…ì˜ ë‹¨ìœ„(unit of work)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í”„ë¡œí† ì½œì…ë‹ˆë‹¤\n",
        "- í˜¸ì¶œ(invoke), ë°°ì¹˜(batch), ìŠ¤íŠ¸ë¦¬ë°(stream), ë³€í™˜(transform), êµ¬ì„±(compose)ì´ ê°€ëŠ¥í•œ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤\n",
        "- LangChainì˜ ëª¨ë“  ì£¼ìš” êµ¬ì„±ìš”ì†Œ(LLM, í”„ë¡¬í”„íŠ¸, ì¶œë ¥ íŒŒì„œ, ë¦¬íŠ¸ë¦¬ë²„ ë“±)ëŠ” Runnable í”„ë¡œí† ì½œì„ êµ¬í˜„í•©ë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Runnableì˜ ì£¼ìš” ë©”ì„œë“œ\n",
        "\n",
        "### í•µì‹¬ ë©”ì„œë“œë“¤\n",
        "| ë©”ì„œë“œ | ì„¤ëª… | ì‚¬ìš© ì˜ˆì‹œ |\n",
        "|--------|------|-----------|\n",
        "| `invoke()` | ë‹¨ì¼ ì…ë ¥ì„ ë°›ì•„ ì¶œë ¥ ìƒì„± | `runnable.invoke(input)` |\n",
        "| `batch()` | ì—¬ëŸ¬ ì…ë ¥ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ | `runnable.batch([input1, input2])` |\n",
        "| `stream()` | ì¶œë ¥ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìƒì„± | `runnable.stream(input)` |\n",
        "| `astream_events()` | ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° (ê³ ê¸‰) | `runnable.astream_events(input)` |\n",
        "\n",
        "### ì…ë ¥/ì¶œë ¥ íƒ€ì… (ì»´í¬ë„ŒíŠ¸ë³„)\n",
        "| ì»´í¬ë„ŒíŠ¸ | ì…ë ¥ íƒ€ì… | ì¶œë ¥ íƒ€ì… |\n",
        "|----------|-----------|-----------|\n",
        "| **Prompt** | ë”•ì…”ë„ˆë¦¬ ê°ì²´ | PromptValue |\n",
        "| **ChatModel** | ë¬¸ìì—´, ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸, PromptValue | ChatMessage |\n",
        "| **LLM** | ë¬¸ìì—´, ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸, PromptValue | ë¬¸ìì—´ |\n",
        "| **OutputParser** | LLM ë˜ëŠ” ChatModelì˜ ì¶œë ¥ | íŒŒì„œì— ë”°ë¼ ë‹¤ë¦„ |\n",
        "| **Retriever** | ë¬¸ìì—´ | Document ë¦¬ìŠ¤íŠ¸ |\n",
        "| **Tool** | ë¬¸ìì—´ ë˜ëŠ” ê°ì²´ | ë„êµ¬ì— ë”°ë¼ ë‹¤ë¦„ |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**ì‹¤ìŠµ ì¤€ë¹„ - ê¸°ë³¸ ì„¤ì •**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "\n",
        "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
        "llm = ChatOllama(\n",
        "    model=\"midm-2.0-base-instruct-q5_k_m\",\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "print(\"ëª¨ë“  ëª¨ë“ˆì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ê¸°ë³¸ Runnable ì‹¤ìŠµ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**invoke() ë©”ì„œë“œ - ë‹¨ì¼ ì…ë ¥ ì²˜ë¦¬**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (Runnable)\n",
        "prompt = PromptTemplate.from_template(\"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ í•œì¤„ë¡œ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”: {topic}\")\n",
        "\n",
        "# 2. ì¶œë ¥ íŒŒì„œ ìƒì„± (Runnable)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Langchainì˜ ê° ì»´í¬ë„ŒíŠ¸ëŠ” Runnable í”„ë¡œí† ì½œì„ êµ¬í˜„í•©ë‹ˆë‹¤\n",
        "# invoke ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰\n",
        "input_data = {\"topic\": \"ë­ì²´ì¸\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "formatted_prompt = prompt.invoke(input_data)\n",
        "print(formatted_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2ë‹¨ê³„: LLM ì‹¤í–‰\n",
        "llm_response = llm.invoke(formatted_prompt)\n",
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3ë‹¨ê³„: ì¶œë ¥ íŒŒì‹±\n",
        "final_output = output_parser.invoke(llm_response)\n",
        "print(final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**ì²´ì¸ êµ¬ì„± - Pipe ì—°ì‚°ì (|) ì‚¬ìš©**\n",
        "- LCELì˜ í•µì‹¬ ê¸°ëŠ¥: Pipe ì—°ì‚°ì(`|`)ë¥¼ ì‚¬ìš©í•œ ì²´ì¸ êµ¬ì„±\n",
        "- ì—¬ëŸ¬ Runnableì„ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°©ë²• 1: Pipe ì—°ì‚°ì ì‚¬ìš©\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# ì²´ì¸ì„ í•˜ë‚˜ì˜ Runnableë¡œ ì‚¬ìš©\n",
        "result = chain.invoke({\"topic\": \"ë¨¸ì‹ ëŸ¬ë‹\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RunnableSequenceë€?\n",
        "- ì—¬ëŸ¬ Runnableì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸ì„ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤\n",
        "- Pipe ì—°ì‚°ì(|)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì œê³µ\n",
        "- ë³µì¡í•œ ì²´ì¸ êµ¬ì„±ì—ì„œ ìœ ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°©ë²• 2: RunnableSequence í´ë˜ìŠ¤ ì‚¬ìš©\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "sequence_chain = RunnableSequence(prompt, llm, output_parser)\n",
        "\n",
        "# ë™ì¼í•œ ê²°ê³¼\n",
        "result2 = sequence_chain.invoke({\"topic\": \"ë”¥ëŸ¬ë‹\"})\n",
        "print(\"RunnableSequence ê²°ê³¼:\")\n",
        "print(result2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**batch() ë©”ì„œë“œ**\n",
        "- ì—¬ëŸ¬ ì…ë ¥ë“¤ (ëª©ë¡)ì— ëŒ€í•´ ì²´ì¸ì„ í˜¸ì¶œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# batch ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì…ë ¥ì„ í•œ ë²ˆì— ì²˜ë¦¬\n",
        "topics = [\n",
        "    {\"topic\": \"ë¸”ë¡ì²´ì¸\"},\n",
        "    {\"topic\": \"ì–‘ìì»´í“¨íŒ…\"},\n",
        "    {\"topic\": \"ì‚¬ë¬¼ì¸í„°ë„·\"}\n",
        "]\n",
        "\n",
        "batch_results = chain.batch(topics)\n",
        "\n",
        "for i, (topic, result) in enumerate(zip(topics, batch_results)):\n",
        "    print(f\"\\n{i+1}. {topic['topic']}:\")\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "**stream() ë©”ì„œë“œ**\n",
        "- LLMì˜ ì¶œë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ë°›ì„ ìˆ˜ ìˆëŠ” ë©”ì„œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stream ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°\n",
        "print(\"ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ (ì‹¤ì‹œê°„ìœ¼ë¡œ í† í°ì´ ìƒì„±ë©ë‹ˆë‹¤):\")\n",
        "\n",
        "for chunk in chain.stream({\"topic\": \"ìì—°ì–´ì²˜ë¦¬\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### RunnableParallel\n",
        "- ì—¬ëŸ¬ Runnableì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‹¤ì–‘í•œ ê´€ì ì˜ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "tech_prompt = PromptTemplate.from_template(\"ê¸°ìˆ ì  ê´€ì ì—ì„œ {topic}ì— ëŒ€í•´ í•œì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")\n",
        "military_prompt = PromptTemplate.from_template(\"êµ°ì‚¬ì  ì—­ëŸ‰ ê´€ì ì—ì„œ {topic}ì˜ í™œìš© ë°©ì•ˆì„ í•œì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")\n",
        "social_prompt = PromptTemplate.from_template(\"ì‚¬íšŒì  ì˜í–¥ ê´€ì ì—ì„œ {topic}ì— ëŒ€í•´ í•œì¤„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")\n",
        "\n",
        "# ë³‘ë ¬ ì²´ì¸ êµ¬ì„±\n",
        "parallel_chain = RunnableParallel(\n",
        "    technical=tech_prompt | llm | output_parser,\n",
        "    military=military_prompt | llm | output_parser,\n",
        "    social=social_prompt | llm | output_parser\n",
        ")\n",
        "\n",
        "# ë³‘ë ¬ ì‹¤í–‰\n",
        "topic_input = {\"topic\": \"ì¸ê³µì§€ëŠ¥\"}\n",
        "results = parallel_chain.invoke(topic_input)\n",
        "\n",
        "print(\"1. ê¸°ìˆ ì  ê´€ì :\")\n",
        "print(results[\"technical\"])\n",
        "print(\"2. êµ°ì‚¬ì  ì—­ëŸ‰ ê´€ì :\")\n",
        "print(results[\"military\"])\n",
        "print(\"3. ì‚¬íšŒì  ì˜í–¥ ê´€ì :\")\n",
        "print(results[\"social\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### RunnableLambda \n",
        "- ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¥¼ Runnableë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.\n",
        "- ì¼ë°˜ì ì¸ Python í•¨ìˆ˜ë¥¼ LangChain ì²´ì¸ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ê°ì‹¸ëŠ” ë˜í¼(wrapper) ê¸°ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. ê°„ë‹¨í•œ ë³€í™˜ í•¨ìˆ˜ë“¤\n",
        "# í…ìŠ¤íŠ¸ì— ì ‘ë‘ì‚¬ ì¶”ê°€\n",
        "def add_prefix(text: str) -> str:\n",
        "    return f\"ğŸ“ í•œì¤„ì„¤ëª…: {text}\"\n",
        "\n",
        "# í•¨ìˆ˜ë“¤ì„ Runnableë¡œ ë³€í™˜\n",
        "prefix_runnable = RunnableLambda(add_prefix)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "sample_text = \"ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤\"\n",
        "\n",
        "print(\"ì›ë³¸ í…ìŠ¤íŠ¸:\", sample_text)\n",
        "print(\"ì ‘ë‘ì‚¬ ì¶”ê°€:\", prefix_runnable.invoke(sample_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Runnable ì¸í„°í˜ì´ìŠ¤ë¡œ ë§Œë“¤ì–´ì§€ê¸° ë•Œë¬¸ì—, chainìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enhanced_chain = (\n",
        "    prompt \n",
        "    | llm \n",
        "    | output_parser \n",
        "    | prefix_runnable  # ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¥¼ ì²´ì¸ì— ì¶”ê°€\n",
        ")\n",
        "\n",
        "result = enhanced_chain.invoke({\"topic\": \"êµ­ë°©ë¶€\"})\n",
        "print(\"í›„ì²˜ë¦¬ê°€ í¬í•¨ëœ ì²´ì¸ ê²°ê³¼:\")\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
